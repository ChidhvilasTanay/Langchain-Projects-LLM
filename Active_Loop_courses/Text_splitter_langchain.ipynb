{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF2vG5LkZ1mj",
        "outputId": "7c1fdd00-73d3-4c84-8d45-d1bfa96fbd1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m583.8/583.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.0.208 deeplake openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd1o-3tXbo1e",
        "outputId": "7a3e7b83-23d3-4d6f-93ac-43772da4388b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/278.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/278.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/278.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m276.5/278.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/Day1.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "7Ug4ZDOnZ4G-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "texts = text_splitter.split_documents(pages)\n",
        "\n",
        "print(texts[0])\n",
        "\n",
        "print (f\"You have {len(texts)} documents\")\n",
        "print (\"Preview:\")\n",
        "print (texts[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS6SZZePbltK",
        "outputId": "4158df58-84fb-4698-edb4-e221840686d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Data  Science  \\nInterview  Questions  \\n(30 days of Interview  Preparation)' metadata={'source': '/content/Day1.pdf', 'page': 0}\n",
            "You have 12 documents\n",
            "Preview:\n",
            "Data  Science  \n",
            "Interview  Questions  \n",
            "(30 days of Interview  Preparation)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(\"/content/Day1.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=50,\n",
        "    chunk_overlap=10,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(pages)\n",
        "for doc in docs:\n",
        "    print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMEBZu-Mcftt",
        "outputId": "87544aa7-f3ca-4f3f-98ee-ab8c20f0cabc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Data  Science  \\nInterview  Questions' metadata={'source': '/content/Day1.pdf', 'page': 0}\n",
            "page_content='(30 days of Interview  Preparation)' metadata={'source': '/content/Day1.pdf', 'page': 0}\n",
            "page_content='INEURON.AI  \\n \\n Page 2' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='Q1.  What is the difference between AI,  Data' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='Data  Science , ML, and DL ?' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='Ans 1 :' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='Artificial Intelligence : AI is purely math and' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='math and scientific exercis e, but when it became' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='it became computa tional , it' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='started to solve human problems formalized into a' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='into a subset of computer science. A rtificial' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='rtificial intelligence has' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='changed the original computational statistics' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='paradigm to the modern idea that machines could' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='could mimic' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='actual human capabilities, such as deci sion' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='deci sion making and perfo rming more “human”' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='“human” tasks. Modern AI into' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='two categories' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='1. General AI - Planning, decision making,' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='making, identifying objects, recognizing sounds,' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='sounds, social &' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='business transactions' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='2. Applied AI - driverless/ Autonomous car or' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='car or machine smartly trade st ocks' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='Machine Learning : Instead of engineers' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='engineers “teaching” or programming computers to' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='to have what they need' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='to carry out tasks, that perhaps computers could' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='could teach themselves – learn something without' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='without being' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='explicitly programmed to do so. ML is a form of' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='a form of AI where  based on more data , and they' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='and they can change' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='actions and response, which will make more effic' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='effic ient, ad aptable and scalable. e .g.,' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='e .g., navigation apps and' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='recommendation engine s. Classified into: -' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='1. Supervised  \\n2. Unsupervised' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='3. Reinforcement learning' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='Data Science : Data science ha s many tools,' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='tools, techniques, and algorithms called from' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='from these fields, plus' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='others –to handle big data' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='The goal of data science, somewhat similar to' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='to machine learning, is to make accurate' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='accurate predictions and to' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='automate and perform transactions in real -time,' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='-time, such as purchasing internet traffic or' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='or automatically' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='generating content.' metadata={'source': '/content/Day1.pdf', 'page': 1}\n",
            "page_content='INEURON.AI  \\n \\n Page 3' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='Data science relies less on math and coding and' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='and more on data and building new systems to' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='to process the' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='data. Relying on the fields of data integration,' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='distributed architecture, aut omated machine' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='machine learning, data' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='visualization, data engineering, and automated' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='automated data -driven decisions, data science' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='science can cover an entire' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='spectrum of data processing, not only the' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='only the algorithms or statistics related to' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='to data.' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='Deep Learning : It is a technique for i' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='for i mplementing ML.' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='ML provides the desired output from a given input' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='input , but DL reads the input and applies it to' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='it to another data.' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='In ML , we can easily classify the flower  based' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='based upon the features . Suppose you want a' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='want a machine to look at' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='an image and determine what  it represents to the' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='to the human eye , whether a face, flower,' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='flower, landscape, truck,' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='building, etc.' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='Machine learning is not sufficient for this task' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='this task because machine learning can only' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='can only produce an output from' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='a data set – whether according to a known' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='a known algorithm or based on the inherent' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='inherent structure of the data. Y ou' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='might be able to use machine learning to' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='to determine whether an image was of an “X” – a' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='“X” – a flower, say – and' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='it would learn and get more accurate. But that' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='But that output is binary (yes/no) and is' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='and is dependent on the' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='algorithm, not the data. In the image recognition' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='ca se, the outcome is not binary and not' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='and not dependent on' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='the algorithm.' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='The n eural network performs MICRO calculations' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='with computational on many layers . Neural' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='. Neural networks' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='also support weighting data for ‘confidence .' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='. These results in a probabilistic system , vs.' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content=', vs. deterministic, and' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='can handle tasks that we think of as requiring' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='requiring more ‘human -like’ judg ment.' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='Q2. What is the difference  between Supervised' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='learning, U nsupervised  learning and' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='Reinforcement  learning ? \\n \\nAns 2 :' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='Ans 2 :  \\nMachine Learning' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='Machine learning is the scient ific study of' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='study of algorithms and statistical models that' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='that computer systems use to' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='effectively perform a specific task without using' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='using explicit instructions, relying on patterns' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='patterns and inference' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='instead.' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='Building a model by learning the patterns of' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='of historical data wi th some relationship' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='between data to make' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='a data -driven prediction.' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='Types of Machine Learning' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='• Supervised Learning  \\n• Unsupervised Learning' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='• Reinforcement Learning' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='Supervised learning' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='In a supervised learning model, the algorithm' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='algorithm learns on a labe led da taset, to' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='taset, to generate reasonable' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='predictions for the response to new data.' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='data.  (Forecasting outcome of new data)' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='• Regression  \\n• Classification' metadata={'source': '/content/Day1.pdf', 'page': 2}\n",
            "page_content='INEURON.AI  \\n \\n Page 4 \\n  \\nUnsupervised learning' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='An unsupervised model, in contrast , provides' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='provides unlabelled data that the algorithm tries' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='tries to make sens e of by' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='extracting features, co -occur rence and' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='rence and underlying patterns on its own. We use' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='We use unsupervised learning for' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='• Clustering  \\n• Anomaly detection' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='• Association  \\n• Autoencoders' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='Reinforcement Learning' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='Reinforcement learning is less supervised and' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='and depends on  the learning agent in determining' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='the output' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='solutions by arriving at different possible ways' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='ways to achieve the best possible solution.' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='Q3. Describe the general architecture of Machine' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='Machine learning .' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='Business understanding : Understand the give use' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content=\"give use cas e, and also , it's good to know more\" metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='know more about the' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='domain for which the use cases are buil t.' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='Data Acquisition and Understanding : Data' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content=': Data gathering from different sources and' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='and understanding the' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='data. Cleaning the data , handling the missing' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='missing data if any , data wrangli ng, and EDA(' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='and EDA( Exp loratory data' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='analysis) .' metadata={'source': '/content/Day1.pdf', 'page': 3}\n",
            "page_content='INEURON.AI  \\n \\n Page 5' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='Modeling:  Feature Engineering  - scaling the' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='the data , feature selection - not all feature s' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='feature s are important. We' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='use the backward elimination method , correlation' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='factors, PCA and domain knowledge to select the' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='features.' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='Model Training  based on trial and error method' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='method or by experience , we select the al' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='the al gorithm and train with' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='the selected features.' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='Model evaluation  Accuracy of the model ,' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='model , confusion matrix and cross -validation.' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='If accuracy is not high , to achi eve higher' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='higher accuracy , we tune  the model...either by' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='by changing the algo rithm' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='used or by feature selection or by g athering' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='athering more data , etc.' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='Deployment  - Once the model has good accuracy ,' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content=', we deplo y the model either in the cloud or' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='cloud or Rasberry' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='py or any other place. Once we deploy , we' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content=', we monitor the performa nce of the model.if its' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='its good...we go live' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='with the model or re iterate the all process unti' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='unti l our model performa nce is good.' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content=\"It's not done yet!!!\" metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='What if , after a few day s, our model performs' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='performs bad ly because of new data . In that' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='. In that case , we do all the' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='process a gain by collecting new data and' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='data and redeploy the model.' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='Q4. What is Linear Regression ? \\n \\nAns 4:' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='Linear Regression tends to establish a' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='a relationship between a depend ent variable(Y)' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='and one or more' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='independ ent variable(X) by finding the best fit' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='best fit of the straight line.' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='The equation for the Linear model is Y = mX+c,' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='Y = mX+c, where m is the slope and c is the' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='c is the intercept' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='In the above diagram, the blue dots we see are' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content=\"see are the distribution of 'y' w.r.t 'x .' There\" metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content=\".' There is no straight line that\" metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='runs through all the data points. So, the' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='So, the objective here is to fit the best fit of' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='fit of a straight line that will try to' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='minimi ze the error between the expected and' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='and actual value .' metadata={'source': '/content/Day1.pdf', 'page': 4}\n",
            "page_content='INEURON.AI  \\n \\n Page 6' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='Q5. OLS Stats Model (Ordinary Least Square)' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='Ans 5:' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='OLS is a stats model, which will help us in' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='us in identifying the more significan t features' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='features that can has an' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='influence on the ou tput. OLS model in python is' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='is  executed  as:' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content=\"lm = smf.ols(formula = 'Sales ~ am+constant',\" metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='data = data).fit() lm.conf_int() lm.summary()' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='And we get the output as below,' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='The higher the t -value for the feature, the more' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='the more significant the feature is to the output' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='output variable . And' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='also, the p -value plays a rule in rejecting the' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='the Null hypothesis(Null hypothesis stating the' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='the features has zero' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='significance on the target variable.).  If the p' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='If the p -value is less than 0.05(95% confidence' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='interval) for a' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='feature, then we  can consider the feature to be' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='to be significant.' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='Q6. What is L1 Regularization (L1 = lasso)  ?' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='Ans 6:' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='The main objective of creating a model(training' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='data) is mak ing sure it fits the data properly' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='properly and reduce' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='the loss. Sometimes the model that is trained' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='trained which will fit the data b ut it may fail' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='may fail and give a poor' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='performance during analyzing of data (test data)' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='data) . This leads to overfitting. Regularization' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='came to' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='overcome overfitting.' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='Lasso Regression ( Least Absolute Shrinkage and' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='and Selection Operator ) adds “ Absolute value of' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='value of' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='magnit ude” of coefficient , as penalty term to' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='term to the loss function.' metadata={'source': '/content/Day1.pdf', 'page': 5}\n",
            "page_content='INEURON.AI  \\n \\n Page 7' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='Lasso shrinks the less important feature’s' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='feature’s coefficient to zero ; thus, removing' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='removing some feature altogether. So,' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='this works well for feature selection in case we' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='case we have a huge number of features.' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='Methods like Cross-validation, Stepwise' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='Stepwise Regression are there  to handle' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='to handle overfitting  and perform feature' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='selection work well with a small set of features' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='features . These techniques are  good when we are' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='we are dealing with a' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='large set of features.' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='Along with sh rinking coefficients,  the lasso' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='the lasso performs feature selection , as well.' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='as well. (Remember the' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='‘selection‘ in the lasso full -form?) Because' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='Because some of the coefficients become exactly' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='exactly zero, which is' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='equivalent to the particular feature being' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='being excluded from the model.' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='Q7. L2 Re gularization(L2 = Ridge Regression)' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='Ans 7:' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='Overfitting happens when the model learns signal' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='signal as well as noise in the training data and' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='data and wouldn’t' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='perform well on new/unseen data on which model' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='model wasn’t trained on.' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='To avoid overfitting your model on training data' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='data like  cross -validation sampling , reducing' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='reducing the number' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='of features, pruning , regularization , etc.' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='So to  avoid overfitting , we perform' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='perform  Regularization .' metadata={'source': '/content/Day1.pdf', 'page': 6}\n",
            "page_content='INEURON.AI  \\n \\n Page 8' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='The Regression model that uses  L2 regularization' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='is called Ridge Regression.' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='The f ormula for Ridge Regression :-' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='Regula rization adds the penalty as model' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='as model complexity increases. The r' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='The r egularization parameter' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='(lambda) penalizes all the parameters except' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='except intercept so that the model generalizes' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='the data and' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='won’t overfit.' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='Ridge regression adds  “squared magnitude of the' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='of the coefficient \" as penalty term to the loss' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='function. Here the box part in the above image' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='image represents the L2 regularization' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='element/term.' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='Lambda is a hyperparameter.' metadata={'source': '/content/Day1.pdf', 'page': 7}\n",
            "page_content='INEURON.AI  \\n \\n Page 9' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='If lambda is zero , then it is equivalent to' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='to OLS. But if lambda is very large , then it' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content=', then it will add too much' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='weight , and it will lead to under -fitting .' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='Ridge regularization  forces the weights to be' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='to be small but does not make them zero  and does' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='and does no t give' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='the sparse solution .' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='Ridge is not robust to outliers  as square terms' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='terms blow  up the error differences of the' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='of the outlie rs, and the' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='regularization term tries to fix it by penalizing' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='the weights' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='Ridge regression performs better when all the' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='all the input features influence the output , and' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content=', and all with  weights' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='are of roughly equal size .' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='L2 regularization can learn complex data patte' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='patte rns.' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='Q8. What is R square(where to use and where not)' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='not) ?' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='Ans 8.' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='R-squared  is a statistical measure of how close' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='how close the data are to the fitted regression' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='line. It is also' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='known as the  coefficient of determination, or' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='or the coefficient of multiple determination for' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='for multiple' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='regre ssion.' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='The definition of R -squared is the  percentage' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='of the response variable variation that is' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='that is explained by a' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='linear model.' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='R-squared = Explained variation / Total variation' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='variation' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='R-squared is always between 0 and 100%.' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='0% indicates that the model explains none  of the' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='of the variability of the response data around' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='around its mean.' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='100% indicates that the model explains all the' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='all the variability of the response data around' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='around its mean.' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='In general, the higher the R -squared, the better' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='better the model fits your data.' metadata={'source': '/content/Day1.pdf', 'page': 8}\n",
            "page_content='INEURON.AI  \\n \\n Page 10' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='There is a  problem w ith the R -Square. The' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='The problem arises when we ask this question to' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='to ourselves.** Is it' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='good to help as many independent variables as' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='as possible?**' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='The answer is No  because we understood that each' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='that each independent variable should have a' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='have a meaningful' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='impact. But, even**  if we add independent' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='variables which are not meaningful**, will it' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='will it improve R -Square' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='value?' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='Yes, this is the basic problem with R -Square.' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='-Square. How many junk independent variables or' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='or important' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='independent variable or impactful independent' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='variable you add to y our model, the R -Squared' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='-Squared value will' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='always increase. It will never decrease with the' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='with the addition of a newly independent variable' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='variable , whether it could' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='be an impactful, non -impactful , or bad' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content=', or bad variable, so we need another way to' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='way to measure equivalent R -' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='Square , which penalizes our model with any junk' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='any junk independent variable.' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='So, we calculate the  Adjusted R -Square  with a' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='with a better adjustment in the formula of' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='of generic R -square.' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='Q9. What is Mean Square  Error?' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='The mean squared error tells you how close a' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='close a regression line is to a set of points. It' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='It does this by' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='taking the distances from the points to the' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='to the regression line (these distances are the' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='are the “errors”) and' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='squaring them.  \\n \\nGiving an intuition' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='The line equation is y=Mx+B . We want to find M' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='to find M (slope)  and B (y-intercept)  that' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='that minimizes the' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='squared e rror.' metadata={'source': '/content/Day1.pdf', 'page': 9}\n",
            "page_content='INEURON.AI  \\n \\n Page 11' metadata={'source': '/content/Day1.pdf', 'page': 10}\n",
            "page_content='Q10. Why S upport  Vector Regre ssion ?' metadata={'source': '/content/Day1.pdf', 'page': 10}\n",
            "page_content='ssion ? Difference between SVR and a simple' metadata={'source': '/content/Day1.pdf', 'page': 10}\n",
            "page_content='a simple regression' metadata={'source': '/content/Day1.pdf', 'page': 10}\n",
            "page_content='model?  \\n \\nAns 10:' metadata={'source': '/content/Day1.pdf', 'page': 10}\n",
            "page_content='In simple linear regression , try to minimi ze' metadata={'source': '/content/Day1.pdf', 'page': 10}\n",
            "page_content='minimi ze the error rate. But in SVR , we try to' metadata={'source': '/content/Day1.pdf', 'page': 10}\n",
            "page_content='try to  fit the er ror within' metadata={'source': '/content/Day1.pdf', 'page': 10}\n",
            "page_content='a certain threshold . \\n \\nMain Concep ts:-' metadata={'source': '/content/Day1.pdf', 'page': 10}\n",
            "page_content='1. Boundary  \\n2. Kernel  \\n3. Support Vector' metadata={'source': '/content/Day1.pdf', 'page': 10}\n",
            "page_content='4. Hyper Plane' metadata={'source': '/content/Day1.pdf', 'page': 10}\n",
            "page_content='Blue line: Hyper Plane; Red Line: Boundary -Line' metadata={'source': '/content/Day1.pdf', 'page': 10}\n",
            "page_content='INEURON.AI  \\n \\n Page 12' metadata={'source': '/content/Day1.pdf', 'page': 11}\n",
            "page_content='Our best fit line is the one w here th e' metadata={'source': '/content/Day1.pdf', 'page': 11}\n",
            "page_content='here th e hyperplane  has the maximum number of' metadata={'source': '/content/Day1.pdf', 'page': 11}\n",
            "page_content='number of points.' metadata={'source': '/content/Day1.pdf', 'page': 11}\n",
            "page_content='We are trying to do here is trying to decide a' metadata={'source': '/content/Day1.pdf', 'page': 11}\n",
            "page_content='decide a decision boundary at ‘ e’ distance from' metadata={'source': '/content/Day1.pdf', 'page': 11}\n",
            "page_content='from the original' metadata={'source': '/content/Day1.pdf', 'page': 11}\n",
            "page_content='hyper plane such that data points closest to the' metadata={'source': '/content/Day1.pdf', 'page': 11}\n",
            "page_content='to the hyper plane or the support vectors are' metadata={'source': '/content/Day1.pdf', 'page': 11}\n",
            "page_content='are within that' metadata={'source': '/content/Day1.pdf', 'page': 11}\n",
            "page_content='boundary line' metadata={'source': '/content/Day1.pdf', 'page': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q nltk"
      ],
      "metadata": {
        "id": "pjwvqiQ0c-T7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JW6sXCBdDm9",
        "outputId": "aa7ed54f-22b2-4d54-f9b6-63e14ca10933"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "\n",
        "with open(\"/content/my_file.txt\",encoding=\"unicode_escape\") as f:\n",
        "  sample_text = f.read()\n",
        "\n",
        "text_splitter = NLTKTextSplitter(chunk_size=200)\n",
        "texts = text_splitter.split_text(sample_text)\n",
        "print(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnyOsFBrdJBX",
        "outputId": "909756a5-e16d-4c03-a361-a37c731d42df"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 328, which is longer than the specified 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.', 'The search giant is launching an API for PaLM alongside a number of AI enterprise tools\\nit says will help businesses \\x93generate text, images, code, videos, audio, and more from\\nsimple natural language prompts.\\x94\\n\\nPaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\\nMeta\\x92s LLaMA family of models.', 'Google first announced PaLM in April 2022.\\n\\nLike other LLMs,\\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\\nediting tasks.', 'You could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample, or you could use it for tasks like summarizing text or even writing code.', '(It\\x92s similar to features Google also announced today for its Workspace apps like Google\\nDocs and Gmail.)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import SpacyTextSplitter\n",
        "\n",
        "with open(\"/content/my_file.txt\",encoding=\"unicode_escape\") as f:\n",
        "  sample_text = f.read()\n",
        "\n",
        "text_splitter = SpacyTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "\n",
        "texts = text_splitter.split_text(sample_text)\n",
        "print(texts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwborKToeXMQ",
        "outputId": "a1df1cb7-2e16-4213-8b52-cdc7af7c817a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\n",
            "\n",
            "\n",
            "Google is offering developers access to one of its most advanced AI language models: PaLM.\n",
            "\n",
            "\n",
            "The search giant is launching an API for PaLM alongside a number of AI enterprise tools\n",
            "it says will help businesses generate text, images, code, videos, audio, and more from\n",
            "simple natural language prompts.\n",
            "\n",
            "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\n",
            "Metas LLaMA family of models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "\n",
        "markdown_text = \"\"\"\n",
        "#\n",
        "\n",
        "# Welcome to My Blog!\n",
        "\n",
        "## Introduction\n",
        "Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python, Java, and JavaScript.\n",
        "\n",
        "Here's a list of my favorite programming languages:\n",
        "\n",
        "1. Python\n",
        "2. JavaScript\n",
        "3. Java\n",
        "\n",
        "You can check out some of my projects on [GitHub](https://github.com).\n",
        "\n",
        "## About this Blog\n",
        "In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\n",
        "\n",
        "Here's a small piece of Python code to say hello:\n",
        "\n",
        "\\``` python\n",
        "def say_hello(name):\n",
        "    print(f\"Hello, {name}!\")\n",
        "\n",
        "say_hello(\"John\")\n",
        "\\```\n",
        "\n",
        "Stay tuned for more updates!\n",
        "\n",
        "## Contact Me\n",
        "Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at johndoe@email.com.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "docs = markdown_splitter.create_documents([markdown_text])\n",
        "\n",
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wmoGEHWfGF2",
        "outputId": "9413196d-e1c1-4d3a-c2cc-f628207fa615"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='# \\n\\n# Welcome to My Blog!', metadata={}), Document(page_content='## Introduction', metadata={}), Document(page_content='Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python,', metadata={}), Document(page_content='Java, and JavaScript.', metadata={}), Document(page_content=\"Here's a list of my favorite programming languages:\\n\\n1. Python\\n2. JavaScript\\n3. Java\", metadata={}), Document(page_content='You can check out some of my projects on [GitHub](https://github.com).', metadata={}), Document(page_content='## About this Blog', metadata={}), Document(page_content=\"In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on\", metadata={}), Document(page_content='the latest technology trends, and occasional book reviews.', metadata={}), Document(page_content=\"Here's a small piece of Python code to say hello:\", metadata={}), Document(page_content='\\\\``` python\\ndef say_hello(name):\\n    print(f\"Hello, {name}!\")\\n\\nsay_hello(\"John\")\\n\\\\', metadata={}), Document(page_content='```\\n\\nStay tuned for more updates!', metadata={}), Document(page_content='## Contact Me', metadata={}), Document(page_content='Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at', metadata={}), Document(page_content='johndoe@email.com.', metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "id": "s9YQ-fImf6EU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "with open(\"/content/my_file.txt\",encoding=\"unicode_escape\") as f:\n",
        "  sample_text = f.read()\n",
        "\n",
        "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=50)\n",
        "\n",
        "\n",
        "texts = text_splitter.split_text(sample_text)\n",
        "print(texts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuQGfy8Pffq_",
        "outputId": "19fc9f6d-ebae-4015-854a-6a2794f45cdc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\n",
            "Google is offering developers access to one of its most advanced AI language models: PaLM.\n",
            "The search giant is launching an API for PaLM alongside a number of AI enterprise tools\n",
            "it says will help businesses generate text, images, code, videos, audio, and more from\n",
            "simple natural language prompts.\n",
            "\n",
            "PaLM is a large language model, or LLM,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1QM4V2eof8BP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}