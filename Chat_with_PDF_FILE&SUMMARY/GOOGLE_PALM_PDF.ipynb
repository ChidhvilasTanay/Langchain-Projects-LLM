{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv6mtLCQ236q",
        "outputId": "30895981-6605-4d94-b6e9-2d827492d529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.310-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.40 (from langchain)\n",
            "  Downloading langsmith-0.0.43-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.310 langsmith-0.0.43 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loAY2FD64JTq",
        "outputId": "49166cd8-b055-41da-b632-6aa909220971"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0.1)\n",
            "Collecting loguru>=0.5.0 (from pinecone-client)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.5.0)\n",
            "Collecting dnspython>=2.0.0 (from pinecone-client)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.6)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
            "Installing collected packages: loguru, dnspython, pinecone-client\n",
            "Successfully installed dnspython-2.4.2 loguru-0.7.2 pinecone-client-2.2.4\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.16.2-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feVA8TeL4VWi",
        "outputId": "25c9aaf3-9504-4b73-cc43-a1dfe6504bf7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.9/267.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "import pinecone\n",
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "Fj5SKAA14cg4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs"
      ],
      "metadata": {
        "id": "fvp1_Ljc5P6d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "xQXqv_s95c2Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKvbbT3k6AL_",
        "outputId": "601d563a-686d-4a0d-8c0e-15bae82ee8d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content=' \\n \\nP a g e  1 | 22 \\n \\n \\n \\n \\n \\n \\nDATA SCIE NCE  \\nINTE RVIEW PREPARATION  \\n(30 Days o f Interview \\nPreparation) \\n \\n \\n#DAY  02  \\n  ', metadata={'source': 'pdfs/Day2.pdf', 'page': 0}), Document(page_content=' \\n \\nP a g e  2 | 22 \\n \\nQ1. What is Logistic Regression ? \\n \\nAnswer: \\nThe logistic regression technique involves the dependent variable , which can be represent ed in the \\nbinary (0 or 1, true or false, yes or no) values, which means that the outcome could only be in either \\none form of two. For example, it can be utilized when we need to find the probability of a \\nsuccessful or fail event.  \\n \\n Logistic Regression is used when  the dependent variable ( target) is categorical.  \\n \\n \\nModel  \\nOutput = 0 or 1  \\nZ = WX + B  \\nhΘ(x) = sigmoid (Z)  \\nhΘ(x) = log(P(X) / 1 - P(X) ) = WX +B  \\n \\n  \\nIf ‘Z’ goes to infinity, Y(predicted) will become 1 , and if ‘Z’ goes to negative infinity, Y(predicted) \\nwill beco me 0.  \\nThe output from the hypothesis is the estimated probability. This is used to infer how confident can \\npredicted value be actual value when given an input X.  \\n \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 1}), Document(page_content=' \\n \\nP a g e  3 | 22 \\n \\nCost Function  \\n \\nCost ( hΘ(x) , Y(Actual)) =    -log (hΘ(x)) if y=1 \\n                                             -log (1 - hΘ(x)) if y=0 \\n \\nThis implementation  is for binary  logistic  regression.  For data with more  than 2 classes,  softmax  re\\ngression  has to be used.  \\n \\n \\n \\nQ2. Difference between logistic and linear regression ? \\n \\nAnswer: \\nLinear and Logistic regression are the most basic form of regression which are commonly used. The \\nessential difference between these two is that Logistic regression is used when the dependent variable \\nis binary . In contrast, Linear regression is used when the dependent variable is continuous , and the \\nnature of the regression  line is linear.  \\n \\nKey Differences between  Linear and Logistic Regression  \\n \\nLinear regression models data using continuous numeric value. As against, logistic regression models \\nthe data in the binary values.  \\nLinear regression requires to establish the line ar relationship among dependent and independent \\nvariable s, whereas it is not necessary for logistic regression.  \\nIn linear regression, the independent variable can be correlated with each other. On the contrary, in \\nthe logistic regression, the variable mu st not be correlated with each other.  \\n \\n \\n \\n \\nQ3. Why we can ’t do a classifica tion problem using Regression ? \\n \\nAnswer :- \\nWith linear regression you fit a polynomial through the data - say, like on the example below , we fit \\na straight line through {tumor size, tumor type} sample set:  \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 2}), Document(page_content=\" \\n \\nP a g e  4 | 22 \\n \\n \\n \\nAbove, malignant tumors get 1 , and non -malignant ones get 0, and the green line is our hypothesis \\nh(x). To make predictions , we may say that for any given tumor size x, if h(x) gets bigger than 0.5 , \\nwe predict malignant tumor s. Otherwise , we predi ct benign ly. \\nIt looks like this way , we could correctly predict every single training set sample, but now let's change \\nthe task a bit.  \\n \\nIntuitively it's clear that all tumors larger certain threshold are malignant. So let's add another sample \\nwith huge tumor si ze, and run linear regression again:  \\n \\n \\n \\nNow our h(x)>0.5 →malignant doesn't work anymore. To keep making correct predictions , we need \\nto change it to h(x)>0.2 or something - but that not how the algorithm should work.  \\n \\nWe cannot change the hypothesis each time  a new sample arrives. Instead, we should learn it off the \\ntraining set data, and then (using the hypothesis we've learned) make correct predictions for the data \\nwe haven't seen before.  \\nLinear regression is unbounded . \\n \\nQ4. What is Decision Tree ? \\n \\nA decision tree is a  type of supervised learning algorithm that can be used in classification as well as \\nregressor problems. The input to a decision tree can be both continuous as well as categorical. The \\ndecision tree works on a n if-then statement. Decision tree tries to solve a problem by using tree \\nrepresentation ( Node and Leaf)  \\nAssumptions while creating a decision tree: 1) Initially all the training set is considered as a root 2) \\nFeature values are preferred to be categorical, if continuous then they are discretized 3) Records are \\n\", metadata={'source': 'pdfs/Day2.pdf', 'page': 3}), Document(page_content=' \\n \\nP a g e  5 | 22 \\n \\ndistributed recursively on the basis of attribute values 4) Which attributes are considered to be in root \\nnode or internal node is done by using a statistical approach . \\n \\n \\n \\n \\n \\nQ5. Entropy, Information Gain, Gini Index, Reducing Impurity ? \\n \\nAnswer: \\nThere are diffe rent attributes which define the split of nodes in a decision tree. There are few \\nalgorithms to find the optimal split . \\n  \\n1) ID3(Iterative Dichotomiser 3) :  This solution uses Entropy and Information gain as metric s \\nto form a better decision tree. The attribute  with the highest information gain is used as a root \\nnode , and a similar approach is followed after that . Entropy is the measure that characterizes \\nthe impurity of an arbitrary collection of examples.  \\n \\n \\n \\n \\nEntropy varies from 0 to 1. 0  if all the data belong t o a single class and 1  if the class distribution is \\nequal. In this way , entropy will give a measure of impurity in the dataset . \\nSteps to decide which attribute to split:  \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 4}), Document(page_content=' \\n \\nP a g e  6 | 22 \\n \\n1. Compute the entropy for the dataset  \\n2. For every attribute:  \\n \\n2.1 Calculate entropy for all categorical values . \\n \\n2.2 Take average information entropy for the attribute . \\n \\n2.3 Calculate gain for the current attribute . \\n3. Pick the attribute with the highest information gain . \\n4. Repeat until we get the desired tree . \\n \\nA leaf node is decided when entropy is zero  \\nInforma tion Ga in = 1 - ∑ (Sb/S)*Entropy  (Sb) \\nSb - Subset, S - entire data  \\n \\n2) CART Algorithm  (Classification and Regression trees):  In CART , we use the GINI index as \\na metric. Gini index is used as a cost function to evaluate split in a dataset  \\nSteps to calculate Gini for a spl it: \\n1. Calculate Gini for subnodes, using formula  sum of the square of probability for success \\nand failure (p2+q2) . \\n2. Calculate Gini for split using weighted Gini score of each node of that split . \\n \\n \\n \\n \\nChoose the split based on higher Gini value  \\n \\n \\n \\nSplit  on Gender:  \\nGini for sub-node  Female  = (0.2)*(0.2)+(0.8)*(0.8)=0.68  \\n     Gini for sub-node  Male  = (0.65)*(0.65)+(0.35)*(0.35)=0.55  \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 5}), Document(page_content=' \\n \\nP a g e  7 | 22 \\n \\n     Weighted  Gini for Split  Gender  = (10/30)*0.68+(20/30)*0.55  = 0.59 \\n \\n \\nSimilar  for Split  on Class:  \\nGini for sub-node  Class  IX = (0.43)*(0.43)+( 0.57)*(0.57)=0.51  \\n     Gini for sub-node  Class  X = (0.56)*(0.56)+(0.44)*(0.44)=0.51  \\n     Weighted  Gini for Split  Class  = (14/30)*0.51+(16/30)*0.51  = 0.51 \\n     \\n    Here  Weighted  Gini is high for gender,  so we consider  splitting based  on gender  \\n \\nQ6. How to control leaf height and Pruning ? \\n \\nAnswer : \\nTo control the leaf size , we can set the parameters: - \\n \\n1. Maximum depth :  \\nMaximum tree depth is a limit to stop the further splitting of nodes when the specified tree depth has \\nbeen reached during the building of the initial dec ision tree.  \\nNEVER use maximum depth to limit the further splitting of nodes. In other words: use the \\nlargest possible value.  \\n \\n \\n2. Minimum split size: \\nMinimum split  size is a limit to stop the further splitting of nodes when the number of observations \\nin the node is lower tha n the minimum split  size. \\nThis is a good way to limit the grow th of the tree. When a leaf contains t oo few observations, further \\nsplitting will result in overfitting (modeling of noise in the data).  \\n \\n3. Minimum leaf size  \\nMinimum leaf  size is a limit to split a n ode when the number of observations in one of the child nodes \\nis lower than the minimum leaf  size. \\nPruning  is mostly done to reduce the chances of overfitting the tree to the training data and reduce \\nthe overall complexity of the tree.  \\n \\nThere are two types o f pruning:  Pre-pruning  and Post-pruning . \\n \\n1. Pre-pruning is also known as  the early stopping criteria . As the name suggests, the criteria \\nare set as parameter values while building the model. The tree stops growing when it meets \\nany of these pre -pruning criteria, o r it discovers the pure classes.  ', metadata={'source': 'pdfs/Day2.pdf', 'page': 6}), Document(page_content=' \\n \\nP a g e  8 | 22 \\n \\n2. In Post -pruning, the idea is to allow the decision tree to grow fully and observe the CP value. \\nNext, we prune/cut the tree with the optimal  CP(Complexity Parameter) value as the \\nparameter.  \\n \\nThe CP (complexity parameter) is u sed to control tree growth. If the cost of adding a variable is \\nhigher , then the value of CP, tree growth stops.  \\n \\n \\n \\n \\n \\n \\n \\nQ7. How to handle a decision tree for numerical and categorical \\ndata? \\nAnswer: \\nDecision trees can handle both categorical and numerical variab les at the same time as features . There \\nis not any problem in doing that.  \\nEvery split in a decision tree is based on a feature.  \\n \\n1. If the feature is categorical, the split is done with the elements belonging to a particular \\nclass . \\n2. If the feature is conti nuous,  the split is done with the elements higher than a threshold.  \\n \\nAt every split, the decision tree will take the best variable at that moment. This will be done according \\nto an impurity measure with the split  branches. And the fact that the variable used t o do split is \\ncategorical or continuous is irrelevant (in fact, decision trees categorize contin uous variables by \\ncreating binary regions with the threshold).  \\nAt last, the good approach is to always convert your categoricals to contin uous \\nusing   LabelEncoder  or OneHotEncoding . \\n \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 7}), Document(page_content=' \\n \\nP a g e  9 | 22 \\n \\n \\n \\nQ8. What is the Random Forest  Algorit hm? \\n \\nAnswer: \\nRandom Forest is an ensemble machine learning algorithm that follows the bagging technique. The \\nbase estimators in the random forest are decision trees. Random forest randomly selects a set of \\nfeatures that are us ed to decide the best split at each node of the decision tree.  \\nLooking at it step -by-step, this is what a random forest model does:  \\n1. Random subsets are created from the original dataset ( bootstrapping ). \\n2. At each node in the decision tree, only a random set o f features are considered to decide the \\nbest split.  \\n3. A decision tree model is fitted on each of the subsets.  \\n4. The final prediction is calculated by averaging the predictions from all decision trees.  \\nTo sum up, the Random forest randomly selects data points and f eatures  and builds multiple trees \\n(Forest) . \\nRandom  Forest is used for feature importance selection. The attribute  (.feature_importances_ ) is \\nused to find feature importance.  \\n \\n \\n \\n \\n \\nSome Important Parameters :- \\n1. n_estimators :- It defines the number of decision tre es to be created in a random forest.  \\n2.  criterion :- \"Gini \" or \" Entropy .\" \\n3. min_samples_split :- Used to define the minimum number of samples required in a leaf \\nnode before a split is attempted  \\n4. max_features : -It defines the maximum number of features allowed for  the split in each \\ndecision tree.  \\n5. n_jobs :- The number of jobs to run in parallel for both fit and predict.  Always keep ( -1) to \\nuse all the cores for parallel processing . \\n \\n \\nQ9. What is Variance and Bias tradeoff ? \\n \\nAnswer : \\nIn predicting models, the prediction error is composed of two different errors  \\n1. Bias \\n2. Variance  ', metadata={'source': 'pdfs/Day2.pdf', 'page': 8}), Document(page_content=\" \\n \\nP a g e  10 | 22 \\n \\n \\nIt is important to understand the variance and bias trade -off which tells about to minimize the Bias \\nand Variance in the predi ction and avoids overfitting & under fitting  of the model . \\n \\nBias: It is the difference between the  expected or average prediction of the model and the correct \\nvalue which we are trying to predict. Imagine  if we are trying to build more than one model by \\ncollecting different data sets , and later on , evaluating the prediction , we may end up by different \\npredic tion for all the models. So, bias is something which measures how far these model prediction \\nfrom the correct prediction. It always leads to a high error in training and test data.  \\n \\nVariance : Variability of a model prediction for a given data point. We can bu ild the model multiple \\ntimes , so the variance is how much the predictions for a given point vary between different \\nrealizations of the model.  \\n \\n \\nFor example : Voting Republican - 13 Voting Democratic - 16 Non -Respondent - 21 Total - 50 \\nThe probability of voting  Republican is 13/(13+16), or 44.8%. We put out our press release that the \\nDemocrats are going to win by over 10 points; but, when the election comes around, it turns out they \\nlose by 10 points. That certainly reflects poorly on us. Where did we g o wrong in our model?  \\nBias scenario's : using a phonebook to select participants in our survey is one of our sources of bias. \\nBy only surveying certain classes of people, it skews the results in a way that will be consistent if we \\nrepeated the entire model  building exercise. Similarly, not following up with respondents is another \\nsource of bias, as it consistently changes the mixture of responses we get. On our bulls -eye diagram , \\nthese move us away from the center of the target, but they would not result in an increased scatter of \\nestimates.  \\nVariance scenario s: the small sample size is a source of variance. If we increased our sample size, \\nthe results would be more consistent each time we repeated the survey and prediction. The results \\nstill might be highly inaccurate due to our large sources of bias, but the variance of predictions will \\nbe reduced  \\n \\n\", metadata={'source': 'pdfs/Day2.pdf', 'page': 9}), Document(page_content=' \\n \\nP a g e  11 | 22 \\n \\n \\n \\n \\n \\nQ10. What are Ensemble Methods ? \\n \\nAnswer  \\n1. Bagging  and Boosting  \\nDecision trees have been around for a long time and also known to suffer from bias and variance. \\nYou will have a large b ias with simple trees and a large variance with complex trees.  \\n \\nEnsemble methods  - which combines several decision trees to produce better predictive \\nperformance than utilizing a single decision tree. The main principle behind the ensemble model is \\nthat a g roup of weak learners come together to form a strong learner.  \\n \\nTwo techniques to perform ensemble decision trees:  \\n1. Bagging  \\n2. Boosting  \\n \\nBagging (Bootstrap Aggregation)  is used when our goal is to reduce the variance of a decision tree. \\nHere the idea is to create sev eral subsets of data from the training sample chosen randomly with \\nreplacement. Now, each collection of subset data is used to train their decision trees. As a result, we \\nend up with an ensemble of different models. Average of all the predictions from differen t trees are \\nused which is more robust than a single decision tree.  \\n \\nBoosting  is another ensemble technique to create a collection of predictors. In this technique, learners \\nare learned sequentially with early learners fitting simple models to the data and t hen analyzing data \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 10}), Document(page_content=' \\n \\nP a g e  12 | 22 \\n \\nfor errors. In other words, we fit consecutive trees (random sample) , and at every step, the goal is to \\nsolve for net error from the prior tree.  \\nWhen a  hypothesis misclassifies an input , its weight is increased , so that the next hypothes is is more \\nlikely to classify it correctly. By combining the whole set at the end converts weak learners into a \\nbetter performing model.  \\n \\nThe different types of boosting algorithms are:  \\n1. AdaBoost  \\n2. Gradient Boosting  \\n3. XGBoost  \\n \\n \\nQ11. What is SVM Classification ? \\n \\nAnswer: \\nSVM or L arge margin classifier is a supervised learning algorithm that uses a powerful technique \\ncalled SVM for classification.  \\nWe have two types of SVM classifier s:  \\n1) Linear SVM : In Linear SVM, the data  points are expected to be sep arated by some appar ent \\ngap. Th erefore, the SVM algorithm predicts a straight hyperplane dividing the two classes. The \\nhyperplane is also called as maximum margin hyperplane  \\n \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 11}), Document(page_content=' \\n \\nP a g e  13 | 22 \\n \\n \\n \\n2) Non -Linear SVM:  It is possible that our data  points are not linearly sep arable in a p -\\ndimensional space, bu t can be linearly sep arable in a higher dimension. Kernel tricks make it \\npossible to draw nonlinear hyperplanes. Some standard kern els are  a) Polynomial Kernel  b) RBF \\nkernel(mostly used) . \\n \\n \\nAdvantages of SVM classifier :    \\n1) SVMs are effective when the num ber of features is quite large.  \\n2) It works effectively even if the number of features is greater than the number of samples.  \\n3) Non -Linear data can also be classified using customized hyperplanes built by using kernel trick.  \\n4) It is a robust model to so lve prediction problems since it maximizes margin.  \\n \\nDisadvantages of SVM classifier:   \\n1) The biggest limitation of the Support Vector Machine is the choice of the kernel. The wrong choice \\nof the kernel can lead to an increase in error percentage.  \\n2) With a great er number of samples, it starts giving poor performances.  \\n3) SVMs have good generalization performance , but they can be extremely slow in the test phase.  \\n4) SVMs have high algorithmic complexity and extensive memory requirements due to the use of \\nquadratic programming.  \\n \\nQ11. What is Naive Ba yes Classification and Gaussian Naive Ba yes \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 12}), Document(page_content=' \\n \\nP a g e  14 | 22 \\n \\n \\nAnswer : \\nBayes’ Theorem  finds the probability of an event occurring given the probability of another event \\nthat has already occurred. Bayes’ theorem is stated mathematically as the following  equation:  \\n \\n  \\nNow, with regards to our dataset, we can apply Bayes’ theorem in following way:  \\nP(y|X) = {P(X|y) P(y)}/{P(X)}  \\nwhere, y is class variable and X is a dependent feature vector (of size n) where:  \\nX = (x_1,x_2,x_3,.....,x_n)  \\n  \\n \\nTo clear, an e xample of a feature vector and corresponding class variable can be: (refer 1st row of \\nthe dataset)  \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 13}), Document(page_content=' \\n \\nP a g e  15 | 22 \\n \\nX = (Rainy, Hot, High, False) y = No So basically, P(X|y) here means, the probability of “Not \\nplaying golf” given that the weather conditions are “Rainy outlook” , “Temperature is hot”, “high \\nhumidity” and “no wind”.  \\n \\nNaive Ba yes Classification : \\n1. We assume that no pair of features are dependent. For example, the temperature being ‘Hot’ \\nhas nothing to do with the humidity , or the outlook being ‘Rainy’ does not affect  the winds. \\nHence, the features are assumed to be independent.  \\n2. Secondly, each feature is given the same weight  (or importance). For example, knowing the \\nonly temperature and humidity alone can’t predict the outcome accurate ly. None of the \\nattributes is irrelevan t and assumed to be contributing equally to the outcome  \\nGaussian Naive Bayes  \\nContinuous values associated with each feature are assumed to be distributed according to a \\nGaussian distribution. A Gaussian distribution is also called Normal distribution. When  plotted, it \\ngives a bell -shaped curve which is symmetric about the mean of the feature values as shown below:  \\n This is as simple as calculating the mean and standard deviation values of each input variable (x) for \\neach class value.  \\nMean (x) = 1/n * sum(x)  \\nWhere n is the number of instances , and x is the values for an input variable in your training data.  \\nWe can calculate the standard deviation using the following equation:  \\nStandard  deviation(x) = sqrt  (1/n * sum(xi -mean(x)^2 ))  \\nWhen to use what ? Standard N aive Bayes only supports categorical features, while Gaussian Naive \\nBayes only supports continuously valued features.  \\n \\n \\nQ12. What is the Confusion Matrix ? \\n \\nAnswer : \\nA confusion matrix is a table that is often used to describe the performance of a classification model \\n(or “classif ier”) on a set of test data for which the true values are known. It allows the visualization \\nof the performance of an algorithm.  \\n \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 14}), Document(page_content=' \\n \\nP a g e  16 | 22 \\n \\nA confusion matrix is a  summary of prediction results  on a classification problem. The number of \\ncorrect and incorrect predicti ons are summarized with count values and broken down by each class.  \\n \\nThis is the key to the confusion matrix.  \\nIt gives us insight not only into the errors being made by a classifier  but, more importantly , the types \\nof errors that are being made.  \\n Here,  \\n\\uf0b7 Class  1: Positive  \\n\\uf0b7 Class 2 : Negative  \\nDefinition of the Terms:  \\n1. Positive (P) : Observation is positive (for example: is an apple).  \\n2. Negative (N) : Observation is not positive (for example: is not an apple).  \\n3. True Positive (TP) : Observation is positive, and is pre dicted to be positive.  \\n4. False Negative (FN) : Observation is positive, but is predicted negative.  \\n5. True Negative (TN) : Observation is negative, and is predicted to be negative.  \\n6. False Positive (FP) : Observation is negative, but is predicted positive.  \\n \\n \\n \\nQ13. What is Accuracy and  Misclassification Rate ? \\n \\nAnswer : \\n \\nAccuracy  \\nAccuracy is defined as the ratio of  the sum of True Positive and True \\nNegative  by Total(TP+TN+FP+FN)  \\n  \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 15}), Document(page_content=' \\n \\nP a g e  17 | 22 \\n \\nHowever,  there are problems  with accuracy. It assumes equal costs for both kinds of \\nerrors.  A 99% accuracy can be excellent, good, mediocre, poor , or terrible depending upon \\nthe problem.  \\n \\n \\n \\n \\nMisclassification Rate  \\n \\nMisclassification Rate is defined as the ratio of  the sum of False Positive and False \\nNegative  by Total(TP+TN+FP+FN)  \\nMisclassification Rate is also called  Error Rate.  \\n \\n \\n \\n \\n \\n \\nQ14. True Positive Rate & True Negative Rate  \\n \\nAnswer : \\n \\nTrue Positive Rate : \\nSensitivity (SN)  is calculated as the number of correct positive predictions divided by the \\ntotal number of positives. It is also called  Recall (REC)  or true positive rate (TPR) . The best \\nsensitivity is 1.0, whereas the worst is 0.0.  \\n \\n \\n \\n \\n \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 16}), Document(page_content=' \\n \\nP a g e  18 | 22 \\n \\n \\n \\n \\nTrue Negative Rate  \\n \\nSpecificity (SP)  is calculated as the number of correct negative predictions divided by the \\ntotal number of negatives. It is also called a true negative rate (TNR). The best speci ficity is \\n1.0, whereas the worst is 0.0.  \\n \\n \\nQ15. What is False Positive Rate & False negative Rate ? \\nFalse Positive Rate  \\nFalse positive rate (FPR) is calculated as the number of incorrect positive predictions divided by the \\ntotal number of negatives. The best false positive rate is 0.0 , whereas the worst is 1.0. It can also be \\ncalculated as 1 – specificity.  \\n \\nFalse Negative Rate  \\nFalse Negative rate (FPR) is calculated as the number of incorrect positive predictions divided by \\nthe total number of positives. The best fa lse negative rate is 0.0 , whereas the worst is 1.0.  \\n \\n \\n \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 17}), Document(page_content=' \\n \\nP a g e  19 | 22 \\n \\nQ16. What are F1 Score, precision and recall ? \\n \\nRecall :- \\nRecall can be defined as the ratio of the total number of correctly classified positive examples \\ndivide to the total number of positive examples.  \\n1. High Recal l indicates the class is correctly recognized (small number of FN).  \\n2. Low Recall indicates the class is incorrectly recognized (large number of FN).  \\n \\nRecall is given by the relation:  \\n  \\n \\nPrecision:  \\nTo get the value of precision , we divide the total number of cor rectly classified positive examples \\nby the total number of predicted positive examples.  \\n1. High Precision indicates an example labeled as positive is indeed positive ( a small number \\nof FP).  \\n2. Low Precision indicates an example labeled as positive is indeed positi ve (large number of \\nFP). \\n \\nThe relation gives precis ion: \\n  \\n \\nRemember :- \\nHigh recall, low precision:  This means that most of the positive examples are correctly recognized \\n(low FN) , but there are a lot of false positives.  \\nLow recall, high precision:  This s hows that we miss a lot of positive examples (high FN) , but those \\nwe predict as positive are indeed positive (low FP).  \\n \\n \\n \\n \\nF-measure/F1 -Score : \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 18}), Document(page_content=' \\n \\nP a g e  20 | 22 \\n \\nSince we have two measures (Precision and Recall) , it helps to have a measurement that represents \\nboth of them. We cal culate an  F-measure , which uses Harmonic Mean in place of Arithmetic \\nMean as it punishes the extreme values more.  \\n \\n \\nThe F -Measure will always be nearer to the smaller value of Precision or Recall.  \\n \\n \\n \\n \\nQ17. What is RandomizedSearchCV ? \\n \\nAnswer : \\nRandomized search CV is used to per form a random  search on hyperparameters. Randomized \\nsearch CV uses a fit and score method, predict proba, decision_func, transform , etc..,  \\nThe parameters of the estimator used to apply these methods are optimized by cross -validated \\nsearch over parameter sett ings.  \\n \\nIn contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of \\nparameter settings is sampled from the specified distributions. The number of parameter settings that \\nare tried is given by n_iter.  \\n \\nCode Example :  \\n \\nclass sklearn.model_ selection.RandomizedSearchCV(estimator, param_distributions, \\nn_iter=10, scoring=None, fit_params=None, n_jobs=None, iid=’warn’, refit=True, \\ncv=’warn’, verbose=0, pre_dispatch=‘2 n_jobs’, random_state=None, error_score=’raise -\\ndeprecating’, return_train_score =’warn’)  \\n \\n \\n \\n \\n \\nQ18. What is GridSearchCV ? \\n \\nAnswer : \\n', metadata={'source': 'pdfs/Day2.pdf', 'page': 19}), Document(page_content=\" \\n \\nP a g e  21 | 22 \\n \\nGrid search is the process of performing hyper parameter tuning to determine the optimal values for \\na given model.  \\nCODE  Example: - \\n \\nfrom sklearn.model_selection import GridSearchCV from sklearn.svm import SVR gsc = \\nGridSearchCV ( estimator=SVR(kernel='rbf'), param_grid={ 'C': [0.1, 1, 100, 1000], 'epsilon': \\n[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10], 'gamma': [0.0001, 0.001, 0.005, 0.1, 1, \\n3, 5] }, cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs= -1) \\n \\nGrid search runs the model on all the possible range of hyperparameter values and outputs the best \\nmodel  \\n \\n \\nQ19. What is BaysianSearch CV? \\n \\nAnswer : \\nBayesian search , in contrast to the grid and random search, keep s track of past evaluation results , \\nwhich they use to form a probabilis tic model mapping hyperparameters to a probability of a score on \\nthe objective function.  \\n \\n \\n \\n \\n \\nCode:  \\nfrom  skopt  import  BayesSearchCV  \\nopt = BayesSearchCV(  \\n    SVC(),  \\n\", metadata={'source': 'pdfs/Day2.pdf', 'page': 20}), Document(page_content=\" \\n \\nP a g e  22 | 22 \\n \\n    { \\n        'C': (1e-6, 1e+6,  'log-uniform'),    \\n        'gamma':  (1e-6, 1e+1,  'log-uniform'),  \\n        'degree':  (1, 8),  # integer  valued  parameter  \\n        'kernel':  ['linear',  'poly',  'rbf'] \\n    }, \\n    n_iter=32,  \\n    cv=3 ) \\n \\n \\n \\nQ20. What is ZCA Whitening ? \\n \\nAnswer : \\nZero Component Analysis:  \\nMaking the co -variance matrix as the Identity matrix is called whitening. T his will remove the first \\nand second -order statistical structure  \\nZCA  transforms  the data to zero mean s and makes  the features  linearly  independ ent of each other  \\nIn some  image  analysis  applications,  especially  when  working  with images  of the color  and tiny typ\\ne, it is frequently  interesting  to apply  some  whitening  to the data before , e.g. training  a classifier.  \", metadata={'source': 'pdfs/Day2.pdf', 'page': 21}), Document(page_content=' \\n \\n \\n \\n \\n \\n \\nData  Science  \\nInterview  Questions  \\n(30 days of Interview  Preparation)', metadata={'source': 'pdfs/Day1.pdf', 'page': 0}), Document(page_content='INEURON.AI  \\n \\n Page 2 \\n  \\nQ1.  What is the difference between AI,  Data  Science , ML, and DL ? \\n \\nAns 1 :  \\n         \\n  \\n \\nArtificial Intelligence : AI is purely math and scientific exercis e, but when it became computa tional , it \\nstarted to solve human problems formalized into a subset of computer science. A rtificial intelligence has \\nchanged the original computational statistics paradigm to the modern idea that machines could mimic \\nactual human capabilities, such as deci sion making and perfo rming more “human” tasks. Modern AI into \\ntwo categories  \\n1. General AI - Planning, decision making, identifying objects, recognizing sounds, social & \\nbusiness transactions  \\n2. Applied AI - driverless/ Autonomous car or machine smartly trade st ocks \\n \\nMachine Learning : Instead of engineers “teaching” or programming computers to have what they need \\nto carry out tasks, that perhaps computers could teach themselves – learn something without being \\nexplicitly programmed to do so. ML is a form of AI where  based on more data , and they can change \\nactions and response, which will make more effic ient, ad aptable and scalable. e .g., navigation apps and \\nrecommendation engine s. Classified into: - \\n1. Supervised  \\n2. Unsupervised  \\n3. Reinforcement learning  \\n \\nData Science : Data science ha s many tools, techniques, and algorithms called from these fields, plus \\nothers –to handle big data  \\nThe goal of data science, somewhat similar to machine learning, is to make accurate predictions and to \\nautomate and perform transactions in real -time, such as purchasing internet traffic or automatically \\ngenerating content.  ', metadata={'source': 'pdfs/Day1.pdf', 'page': 1}), Document(page_content='INEURON.AI  \\n \\n Page 3 \\n Data science relies less on math and coding and more on data and building new systems to process the \\ndata. Relying on the fields of data integration, distributed architecture, aut omated machine learning, data \\nvisualization, data engineering, and automated data -driven decisions, data science can cover an entire \\nspectrum of data processing, not only the algorithms or statistics related to data.  \\n \\nDeep Learning : It is a technique for i mplementing ML.  \\nML provides the desired output from a given input , but DL reads the input and applies it to another data.  \\nIn ML , we can easily classify the flower  based upon the features . Suppose you want a machine to look at \\nan image and determine what  it represents to the human eye , whether a face, flower, landscape, truck, \\nbuilding, etc.  \\nMachine learning is not sufficient for this task because machine learning can only produce an output from \\na data set – whether according to a known algorithm or based on the inherent structure of the data. Y ou \\nmight be able to use machine learning to determine whether an image was of an “X” – a flower, say – and \\nit would learn and get more accurate. But that output is binary (yes/no) and is dependent on the \\nalgorithm, not the data. In the image recognition ca se, the outcome is not binary and not dependent on \\nthe algorithm.  \\nThe n eural network performs MICRO calculations with computational on many layers . Neural networks \\nalso support weighting data for ‘confidence . These results in a probabilistic system , vs. deterministic, and \\ncan handle tasks that we think of as requiring more ‘human -like’ judg ment.  \\n \\n \\nQ2. What is the difference  between Supervised learning, U nsupervised  learning and \\nReinforcement  learning ? \\n \\nAns 2 :  \\nMachine Learning  \\nMachine learning is the scient ific study of algorithms and statistical models that computer systems use to \\neffectively perform a specific task without using explicit instructions, relying on patterns and inference \\ninstead.  \\nBuilding a model by learning the patterns of historical data wi th some relationship between data to make \\na data -driven prediction.  \\n \\nTypes of Machine Learning  \\n• Supervised Learning  \\n• Unsupervised Learning  \\n• Reinforcement Learning  \\n \\nSupervised learning  \\nIn a supervised learning model, the algorithm learns on a labe led da taset, to generate reasonable \\npredictions for the response to new data.  (Forecasting outcome of new data)  \\n• Regression  \\n• Classification  \\n ', metadata={'source': 'pdfs/Day1.pdf', 'page': 2}), Document(page_content=\"INEURON.AI  \\n \\n Page 4 \\n  \\nUnsupervised learning  \\nAn unsupervised model, in contrast , provides unlabelled data that the algorithm tries to make sens e of by \\nextracting features, co -occur rence and underlying patterns on its own. We use unsupervised learning for  \\n• Clustering  \\n• Anomaly detection  \\n• Association  \\n• Autoencoders  \\nReinforcement Learning  \\nReinforcement learning is less supervised and depends on  the learning agent in determining the output \\nsolutions by arriving at different possible ways to achieve the best possible solution.  \\n \\nQ3. Describe the general architecture of Machine learning . \\n \\n          \\n  \\n \\n \\nBusiness understanding : Understand the give use cas e, and also , it's good to know more about the \\ndomain for which the use cases are buil t. \\n \\nData Acquisition and Understanding : Data gathering from different sources and understanding the \\ndata. Cleaning the data , handling the missing data if any , data wrangli ng, and EDA( Exp loratory data \\nanalysis) . \\n \\n \", metadata={'source': 'pdfs/Day1.pdf', 'page': 3}), Document(page_content=\"INEURON.AI  \\n \\n Page 5 \\n Modeling:  Feature Engineering  - scaling the data , feature selection - not all feature s are important. We \\nuse the backward elimination method , correlation factors, PCA and domain knowledge to select the \\nfeatures.  \\nModel Training  based on trial and error method or by experience , we select the al gorithm and train with \\nthe selected features.  \\nModel evaluation  Accuracy of the model , confusion matrix and cross -validation.  \\nIf accuracy is not high , to achi eve higher accuracy , we tune  the model...either by changing the algo rithm \\nused or by feature selection or by g athering more data , etc. \\nDeployment  - Once the model has good accuracy , we deplo y the model either in the cloud or Rasberry \\npy or any other place. Once we deploy , we monitor the performa nce of the model.if its good...we go live \\nwith the model or re iterate the all process unti l our model performa nce is good.  \\nIt's not done yet!!!  \\nWhat if , after a few day s, our model performs bad ly because of new data . In that case , we do all the \\nprocess a gain by collecting new data and redeploy the model.  \\n \\nQ4. What is Linear Regression ? \\n \\nAns 4: \\nLinear Regression tends to establish a relationship between a depend ent variable(Y) and one or more \\nindepend ent variable(X) by finding the best fit of the straight line.  \\nThe equation for the Linear model is Y = mX+c, where m is the slope and c is the intercept  \\n \\n \\nIn the above diagram, the blue dots we see are the distribution of 'y' w.r.t 'x .' There is no straight line that \\nruns through all the data points. So, the objective here is to fit the best fit of a straight line that will try to \\nminimi ze the error between the expected and actual value . \\n \\n \", metadata={'source': 'pdfs/Day1.pdf', 'page': 4}), Document(page_content=\"INEURON.AI  \\n \\n Page 6 \\n  \\nQ5. OLS Stats Model (Ordinary Least Square)  \\n \\nAns 5: \\nOLS is a stats model, which will help us in identifying the more significan t features that can has an \\ninfluence on the ou tput. OLS model in python is  executed  as: \\nlm = smf.ols(formula = 'Sales ~ am+constant', data = data).fit() lm.conf_int() lm.summary()  \\nAnd we get the output as below,  \\n \\n \\nThe higher the t -value for the feature, the more significant the feature is to the output variable . And \\nalso, the p -value plays a rule in rejecting the Null hypothesis(Null hypothesis stating the features has zero \\nsignificance on the target variable.).  If the p -value is less than 0.05(95% confidence interval) for a \\nfeature, then we  can consider the feature to be significant.  \\n \\n \\nQ6. What is L1 Regularization (L1 = lasso)  ? \\n \\nAns 6: \\nThe main objective of creating a model(training data) is mak ing sure it fits the data properly and reduce \\nthe loss. Sometimes the model that is trained which will fit the data b ut it may fail and give a poor \\nperformance during analyzing of data (test data) . This leads to overfitting. Regularization came to \\novercome overfitting.  \\nLasso Regression ( Least Absolute Shrinkage and Selection Operator ) adds “ Absolute value of \\nmagnit ude” of coefficient , as penalty term to the loss function.  \", metadata={'source': 'pdfs/Day1.pdf', 'page': 5}), Document(page_content='INEURON.AI  \\n \\n Page 7 \\n Lasso shrinks the less important feature’s coefficient to zero ; thus, removing some feature altogether. So, \\nthis works well for feature selection in case we have a huge number of features.  \\n \\n \\n \\nMethods like Cross-validation, Stepwise Regression are there  to handle overfitting  and perform feature \\nselection work well with a small set of features . These techniques are  good when we are dealing with a \\nlarge set of features.  \\nAlong with sh rinking coefficients,  the lasso performs feature selection , as well. (Remember the \\n‘selection‘ in the lasso full -form?) Because some of the coefficients become exactly zero, which is \\nequivalent to the particular feature being excluded from the model.  \\n \\nQ7. L2 Re gularization(L2 = Ridge Regression)  \\n \\nAns 7: \\n \\n \\nOverfitting happens when the model learns signal as well as noise in the training data and wouldn’t \\nperform well on new/unseen data on which model wasn’t trained on.  \\nTo avoid overfitting your model on training data like  cross -validation sampling , reducing the number \\nof features, pruning , regularization , etc. \\nSo to  avoid overfitting , we perform  Regularization . \\n ', metadata={'source': 'pdfs/Day1.pdf', 'page': 6}), Document(page_content='INEURON.AI  \\n \\n Page 8 \\n  \\n \\nThe Regression model that uses  L2 regularization is called Ridge Regression.  \\nThe f ormula for Ridge Regression :-  \\n \\n \\nRegula rization adds the penalty as model complexity increases. The r egularization parameter \\n(lambda) penalizes all the parameters except intercept so that the model generalizes the data and \\nwon’t overfit.  \\nRidge regression adds  “squared magnitude of the coefficient \" as penalty term to the loss \\nfunction. Here the box part in the above image represents the L2 regularization element/term.  \\n \\n \\nLambda is a hyperparameter.  ', metadata={'source': 'pdfs/Day1.pdf', 'page': 7}), Document(page_content='INEURON.AI  \\n \\n Page 9 \\n If lambda is zero , then it is equivalent to OLS. But if lambda is very large , then it will add too much \\nweight , and it will lead to under -fitting . \\nRidge regularization  forces the weights to be small but does not make them zero  and does no t give \\nthe sparse solution . \\nRidge is not robust to outliers  as square terms blow  up the error differences of the outlie rs, and the \\nregularization term tries to fix it by penalizing the weights  \\nRidge regression performs better when all the input features influence the output , and all with  weights \\nare of roughly equal size . \\nL2 regularization can learn complex data patte rns. \\n \\nQ8. What is R square(where to use and where not) ? \\nAns 8. \\nR-squared  is a statistical measure of how close the data are to the fitted regression line. It is also \\nknown as the  coefficient of determination, or the coefficient of multiple determination for multiple \\nregre ssion.  \\nThe definition of R -squared is the  percentage of the response variable variation that is explained by a \\nlinear model.  \\nR-squared = Explained variation / Total variation  \\nR-squared is always between 0 and 100%.  \\n0% indicates that the model explains none  of the variability of the response data around its mean.  \\n100% indicates that the model explains all the variability of the response data around its mean.  \\nIn general, the higher the R -squared, the better the model fits your data.  \\n \\n \\n ', metadata={'source': 'pdfs/Day1.pdf', 'page': 8}), Document(page_content='INEURON.AI  \\n \\n Page 10 \\n There is a  problem w ith the R -Square. The problem arises when we ask this question to ourselves.** Is it \\ngood to help as many independent variables as possible?**  \\nThe answer is No  because we understood that each independent variable should have a meaningful \\nimpact. But, even**  if we add independent variables which are not meaningful**, will it improve R -Square \\nvalue?  \\nYes, this is the basic problem with R -Square. How many junk independent variables or important \\nindependent variable or impactful independent variable you add to y our model, the R -Squared value will \\nalways increase. It will never decrease with the addition of a newly independent variable , whether it could \\nbe an impactful, non -impactful , or bad variable, so we need another way to measure equivalent R -\\nSquare , which penalizes our model with any junk independent variable.   \\nSo, we calculate the  Adjusted R -Square  with a better adjustment in the formula of generic R -square.  \\n \\n \\nQ9. What is Mean Square  Error? \\nThe mean squared error tells you how close a regression line is to a set of points. It does this by \\ntaking the distances from the points to the regression line (these distances are the “errors”) and \\nsquaring them.  \\n \\nGiving an intuition  \\n \\n \\n \\nThe line equation is y=Mx+B . We want to find M (slope)  and B (y-intercept)  that minimizes the \\nsquared e rror. ', metadata={'source': 'pdfs/Day1.pdf', 'page': 9}), Document(page_content='INEURON.AI  \\n \\n Page 11 \\n  \\n \\n \\n \\nQ10. Why S upport  Vector Regre ssion ? Difference between SVR and a simple regression \\nmodel?  \\n \\nAns 10: \\nIn simple linear regression , try to minimi ze the error rate. But in SVR , we try to  fit the er ror within \\na certain threshold . \\n \\nMain Concep ts:- \\n1. Boundary  \\n2. Kernel  \\n3. Support Vector  \\n4. Hyper Plane  \\n \\n \\n \\n \\nBlue line: Hyper Plane; Red Line: Boundary -Line \\n ', metadata={'source': 'pdfs/Day1.pdf', 'page': 10}), Document(page_content='INEURON.AI  \\n \\n Page 12 \\n Our best fit line is the one w here th e hyperplane  has the maximum number of points.  \\nWe are trying to do here is trying to decide a decision boundary at ‘ e’ distance from the original \\nhyper plane such that data points closest to the hyper plane or the support vectors are within that \\nboundary line  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', metadata={'source': 'pdfs/Day1.pdf', 'page': 11})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")"
      ],
      "metadata": {
        "id": "RUrqpwi26BK6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = text_splitter.split_documents(data)\n"
      ],
      "metadata": {
        "id": "XSV0AS1E7JQH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "031mOHjJ7RRG",
        "outputId": "87ab65db-75d7-4dfc-dab2-d55b3bb2f53d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_chunks[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKaDgDvh7TVY",
        "outputId": "b6cea30e-c11d-46e8-9139-10f1c2410fd9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='P a g e  3 | 22 \\n \\nCost Function  \\n \\nCost ( hΘ(x) , Y(Actual)) =    -log (hΘ(x)) if y=1 \\n                                             -log (1 - hΘ(x)) if y=0 \\n \\nThis implementation  is for binary  logistic  regression.  For data with more  than 2 classes,  softmax  re\\ngression  has to be used.  \\n \\n \\n \\nQ2. Difference between logistic and linear regression ? \\n \\nAnswer: \\nLinear and Logistic regression are the most basic form of regression which are commonly used. The \\nessential difference between these two is that Logistic regression is used when the dependent variable \\nis binary . In contrast, Linear regression is used when the dependent variable is continuous , and the \\nnature of the regression  line is linear.  \\n \\nKey Differences between  Linear and Logistic Regression  \\n \\nLinear regression models data using continuous numeric value. As against, logistic regression models \\nthe data in the binary values.' metadata={'source': 'pdfs/Day2.pdf', 'page': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "L4HZVRi67X-8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = GooglePalmEmbeddings()"
      ],
      "metadata": {
        "id": "z8SIiXeD7ntO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_result = embedding.embed_query(\"Hello World\")\n",
        "print(len(query_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YNdjdlV7sH9",
        "outputId": "581c06d3-00d1-4fbc-90ee-1934ddf7aa2e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY',\"814b0486-9f63-4cc8-aea6-abaefa4b0d7b\")\n",
        "PINECONE_API_ENV = os.environ.get(\"PINECONE_API_ENV\",\"gcp-starter\")"
      ],
      "metadata": {
        "id": "FIj6wYoF72xj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,\n",
        "    environment=PINECONE_API_ENV\n",
        ")\n",
        "\n",
        "index_name=\"anantha\""
      ],
      "metadata": {
        "id": "Sb81mVyk9CDN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = Pinecone.from_texts([t.page_content for t in text_chunks],embedding=embedding,index_name=index_name)"
      ],
      "metadata": {
        "id": "SmQZFXeH9OTb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docsearch = Pinecone.from_existing_index(index_name, embeddings)"
      ],
      "metadata": {
        "id": "7mbx98lX-hpA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Data Science?\""
      ],
      "metadata": {
        "id": "aj1fQFCt-ktA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = docsearch.similarity_search(query,k=4)"
      ],
      "metadata": {
        "id": "vmKYR5Di_JZN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR4CvWcG_SSh",
        "outputId": "0fc17af6-c00f-4bcc-c0df-ffb8a6795974"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Data  Science  \\nInterview  Questions  \\n(30 days of Interview  Preparation)'),\n",
              " Document(page_content='INEURON.AI  \\n \\n Page 2 \\n  \\nQ1.  What is the difference between AI,  Data  Science , ML, and DL ? \\n \\nAns 1 :  \\n         \\n  \\n \\nArtificial Intelligence : AI is purely math and scientific exercis e, but when it became computa tional , it \\nstarted to solve human problems formalized into a subset of computer science. A rtificial intelligence has \\nchanged the original computational statistics paradigm to the modern idea that machines could mimic \\nactual human capabilities, such as deci sion making and perfo rming more “human” tasks. Modern AI into \\ntwo categories  \\n1. General AI - Planning, decision making, identifying objects, recognizing sounds, social & \\nbusiness transactions  \\n2. Applied AI - driverless/ Autonomous car or machine smartly trade st ocks \\n \\nMachine Learning : Instead of engineers “teaching” or programming computers to have what they need \\nto carry out tasks, that perhaps computers could teach themselves – learn something without being'),\n",
              " Document(page_content='INEURON.AI  \\n \\n Page 3 \\n Data science relies less on math and coding and more on data and building new systems to process the \\ndata. Relying on the fields of data integration, distributed architecture, aut omated machine learning, data \\nvisualization, data engineering, and automated data -driven decisions, data science can cover an entire \\nspectrum of data processing, not only the algorithms or statistics related to data.  \\n \\nDeep Learning : It is a technique for i mplementing ML.  \\nML provides the desired output from a given input , but DL reads the input and applies it to another data.  \\nIn ML , we can easily classify the flower  based upon the features . Suppose you want a machine to look at \\nan image and determine what  it represents to the human eye , whether a face, flower, landscape, truck, \\nbuilding, etc.  \\nMachine learning is not sufficient for this task because machine learning can only produce an output from'),\n",
              " Document(page_content='Machine Learning : Instead of engineers “teaching” or programming computers to have what they need \\nto carry out tasks, that perhaps computers could teach themselves – learn something without being \\nexplicitly programmed to do so. ML is a form of AI where  based on more data , and they can change \\nactions and response, which will make more effic ient, ad aptable and scalable. e .g., navigation apps and \\nrecommendation engine s. Classified into: - \\n1. Supervised  \\n2. Unsupervised  \\n3. Reinforcement learning  \\n \\nData Science : Data science ha s many tools, techniques, and algorithms called from these fields, plus \\nothers –to handle big data  \\nThe goal of data science, somewhat similar to machine learning, is to make accurate predictions and to \\nautomate and perform transactions in real -time, such as purchasing internet traffic or automatically \\ngenerating content.')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GooglePalm(temperature=0.7)"
      ],
      "metadata": {
        "id": "dsOr42Gb_Toh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QA = RetrievalQA.from_chain_type(llm=llm,chain_type=\"stuff\",retriever=docsearch.as_retriever())"
      ],
      "metadata": {
        "id": "aLqgryrv_ZWL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QA.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "r3D7vuNIA5Gd",
        "outputId": "6d48a195-8d9c-48a6-f3ea-7a9800de27d3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Data science has many tools, techniques, and algorithms called from these fields, plus others –to handle big data  The goal of data science, somewhat similar to machine learning, is to make accurate predictions and to  automate and perform transactions in real -time, such as purchasing internet traffic or automatically  generating content.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"Why we should study Data Science?\"\n"
      ],
      "metadata": {
        "id": "PGCjcUriBBr6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QA.run(query1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rl5tQA7vBYuI",
        "outputId": "4ffd44f0-4762-4c42-b30f-9478586f89f8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'To make accurate predictions and to automate and perform transactions in real-time'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = \"Difference between Machine learning and Data science\""
      ],
      "metadata": {
        "id": "_O5EceveBa06"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QA.run(query2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jVxn8eNyBlQk",
        "outputId": "4f4061ed-8e0f-45e0-f98e-7cbcdccb7534"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Data science relies less on math and coding and more on data and building new systems to process the data.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  user_input = input(\"Input Prompt:\")\n",
        "  if user_input=='exit':\n",
        "    print(\"Exit...\")\n",
        "    sys.exit()\n",
        "  if user_input=='':\n",
        "    continue\n",
        "\n",
        "  result = QA({'query':user_input})\n",
        "  print(\"Answer : \",{result['result']})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "nhazgbcuBnSU",
        "outputId": "e29171ba-9ba6-408b-cfe6-97b09687d6a6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Prompt:Why we should study Data Science?\n",
            "Answer :  {'To make accurate predictions and to automate and perform transactions in real-time'}\n",
            "Input Prompt:Difference between Machine learning and Data science\n",
            "Answer :  {'Machine learning is a subfield of data science.\\n\\nMachine learning relies less on math and coding and more on data and building new systems to process the \\ndata. Relying on the fields of data integration, distributed architecture, aut omated machine learning, data \\nvisualization, data engineering, and automated data -driven decisions, data science can cover an entire \\nspectrum of data processing, not only the algorithms or statistics related to data.'}\n",
            "Input Prompt:exit\n",
            "Exit...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QP6RndjbCWRy"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZpgpS3ZaDNN5"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GchzMBJpDeD-"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iFs4yZt_EEBp"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9o43ckDAEY3B"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e5nYm-kgE2F3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}