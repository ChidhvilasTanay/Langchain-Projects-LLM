{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcM9hWOeRnLa",
        "outputId": "312d2793-a7d9-44a7-a113-1e5a5c084c3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio openai pypdf tiktoken langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "openai_key=\"\"\n",
        "os.environ['OPENAI_API_KEY']= openai_key"
      ],
      "metadata": {
        "id": "RmVq9LY6SSVG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "import gradio as gr\n",
        "import tiktoken\n",
        "import openai"
      ],
      "metadata": {
        "id": "iEeV3t_eShr_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN9z2__KSoNm",
        "outputId": "ffcab218-3ab1-4854-aff9-521318981c64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-19 11:54:55--  https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.182.34.216, 52.216.85.149, 52.216.97.189, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.182.34.216|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3651423 (3.5M) [application/pdf]\n",
            "Saving to: ‘2023_GPT4All_Technical_Report.pdf’\n",
            "\n",
            "2023_GPT4All_Techni 100%[===================>]   3.48M  8.17MB/s    in 0.4s    \n",
            "\n",
            "2023-09-19 11:54:55 (8.17 MB/s) - ‘2023_GPT4All_Technical_Report.pdf’ saved [3651423/3651423]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader=PyPDFLoader(\"/content/2023_GPT4All_Technical_Report.pdf\")"
      ],
      "metadata": {
        "id": "ccRopu0DSxfb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs=loader.load()"
      ],
      "metadata": {
        "id": "n1ZbkFhIS3oA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(docs)"
      ],
      "metadata": {
        "id": "PqOFKU-DS58l"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
      ],
      "metadata": {
        "id": "MZfZ6uKoS7Z0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts=text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "r8kh_uMNTBMQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.5)"
      ],
      "metadata": {
        "id": "79hAQQ0bTDNT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain=load_summarize_chain(llm, chain_type='map_reduce', verbose=True)"
      ],
      "metadata": {
        "id": "_-PaY-iETJNP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lMht5KJqTLJc",
        "outputId": "7a7f7aad-acf1-4917-ab3c-d31618f91c55"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"GPT4All: Training an Assistant-style Chatbot with Large Scale Data\n",
            "Distillation from GPT-3.5-Turbo\n",
            "Yuvanesh Anand\n",
            "yuvanesh@nomic.aiZach Nussbaum\n",
            "zanussbaum@gmail.com\n",
            "Brandon Duderstadt\n",
            "brandon@nomic.aiBenjamin Schmidt\n",
            "ben@nomic.aiAndriy Mulyar\n",
            "andriy@nomic.ai\n",
            "Abstract\n",
            "This preliminary technical report describes the\n",
            "development of GPT4All, a chatbot trained\n",
            "over a massive curated corpus of assistant in-\n",
            "teractions including word problems, story de-\n",
            "scriptions, multi-turn dialogue, and code. We\n",
            "openly release the collected data, data cura-\n",
            "tion procedure, training code, and final model\n",
            "weights to promote open research and repro-\n",
            "ducibility. Additionally, we release quantized\n",
            "4-bit versions of the model allowing virtually\n",
            "anyone to run the model on CPU.\n",
            "1 Data Collection and Curation\n",
            "We collected roughly one million prompt-\n",
            "response pairs using the GPT-3.5-Turbo OpenAI\n",
            "API between March 20, 2023 and March 26th,\n",
            "2023. To do this, we first gathered a diverse sam-\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"We collected roughly one million prompt-\n",
            "response pairs using the GPT-3.5-Turbo OpenAI\n",
            "API between March 20, 2023 and March 26th,\n",
            "2023. To do this, we first gathered a diverse sam-\n",
            "ple of questions/prompts by leveraging three pub-\n",
            "licly available datasets:\n",
            "• The unified chip2 subset of LAION OIG.\n",
            "• Coding questions with a random sub-sample\n",
            "of Stackoverflow Questions\n",
            "• Instruction-tuning with a sub-sample of Big-\n",
            "science/P3\n",
            "We chose to dedicate substantial attention to data\n",
            "preparation and curation based on commentary in\n",
            "the Stanford Alpaca project (Taori et al., 2023).\n",
            "Upon collection of the initial dataset of prompt-\n",
            "generation pairs, we loaded data into Atlas for data\n",
            "curation and cleaning. With Atlas, we removed all\n",
            "examples where GPT-3.5-Turbo failed to respond\n",
            "to prompts and produced malformed output. This\n",
            "reduced our total number of examples to 806,199\n",
            "high-quality prompt-generation pairs. Next, we\n",
            "decided to remove the entire Bigscience/P3 sub-\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"to prompts and produced malformed output. This\n",
            "reduced our total number of examples to 806,199\n",
            "high-quality prompt-generation pairs. Next, we\n",
            "decided to remove the entire Bigscience/P3 sub-\n",
            "set from the final training dataset due to its very\n",
            "Figure 1: TSNE visualization of the candidate training\n",
            "data (Red: Stackoverflow, Orange: chip2, Blue: P3).\n",
            "The large blue balls (e.g. indicated by the red arrow)\n",
            "are highly homogeneous prompt-response pairs.\n",
            "low output diversity; P3 contains many homoge-\n",
            "neous prompts which produce short and homoge-\n",
            "neous responses from GPT-3.5-Turbo. This exclu-\n",
            "sion produces a final subset containing 437,605\n",
            "prompt-generation pairs, which is visualized in\n",
            "Figure 2. You can interactively explore the dataset\n",
            "at each stage of cleaning at the following links:\n",
            "• Cleaned with P3\n",
            "• Cleaned without P3 (Final Training Dataset)\n",
            "2 Model Training\n",
            "We train several models finetuned from an in-\n",
            "stance of LLaMA 7B (Touvron et al., 2023).\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"• Cleaned with P3\n",
            "• Cleaned without P3 (Final Training Dataset)\n",
            "2 Model Training\n",
            "We train several models finetuned from an in-\n",
            "stance of LLaMA 7B (Touvron et al., 2023).\n",
            "The model associated with our initial public re-\n",
            "lease is trained with LoRA (Hu et al., 2021)\n",
            "on the 437,605 post-processed examples for four\n",
            "epochs. Detailed model hyper-parameters and\n",
            "training code can be found in the associated repos-\n",
            "itory and model training log.\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"(a) TSNE visualization of the final training data, ten-colored\n",
            "by extracted topic.\n",
            "(b) Zoomed in view of Figure 2a. The region displayed con-\n",
            "tains generations related to personal health and wellness.\n",
            "Figure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\n",
            "2.1 Reproducibility\n",
            "We release all data (including unused P3 genera-\n",
            "tions), training code, and model weights for the\n",
            "community to build upon. Please check the Git\n",
            "repository for the most up-to-date data, training\n",
            "details and checkpoints.\n",
            "2.2 Costs\n",
            "We were able to produce these models with about\n",
            "four days work, $800 in GPU costs (rented from\n",
            "Lambda Labs and Paperspace) including several\n",
            "failed trains, and $500 in OpenAI API spend.\n",
            "Our released model, gpt4all-lora, can be trained in\n",
            "about eight hours on a Lambda Labs DGX A100\n",
            "8x 80GB for a total cost of $100 .\n",
            "3 Evaluation\n",
            "We perform a preliminary evaluation of our model\n",
            "using the human evaluation data from the Self-\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"about eight hours on a Lambda Labs DGX A100\n",
            "8x 80GB for a total cost of $100 .\n",
            "3 Evaluation\n",
            "We perform a preliminary evaluation of our model\n",
            "using the human evaluation data from the Self-\n",
            "Instruct paper (Wang et al., 2022). We report the\n",
            "ground truth perplexity of our model against what\n",
            "is, to our knowledge, the best openly available\n",
            "alpaca-lora model, provided by user chainyo on\n",
            "huggingface. We find that all models have very\n",
            "large perplexities on a small number of tasks, and\n",
            "report perplexities clipped to a maximum of 100.\n",
            "Models finetuned on this collected dataset ex-\n",
            "hibit much lower perplexity in the Self-Instruct\n",
            "evaluation compared to Alpaca. This evaluation is\n",
            "in no way exhaustive and further evaluation work\n",
            "Figure 3: Model Perplexities. Lower is better. Our\n",
            "models achieve stochastically lower ground truth per-\n",
            "plexities than alpaca-lora.\n",
            "remains. We welcome the reader to run the model\n",
            "locally on CPU (see Github for files) and get a\n",
            "qualitative sense of what it can do.\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"plexities than alpaca-lora.\n",
            "remains. We welcome the reader to run the model\n",
            "locally on CPU (see Github for files) and get a\n",
            "qualitative sense of what it can do.\n",
            "4 Use Considerations\n",
            "The authors release data and training details in\n",
            "hopes that it will accelerate open LLM research,\n",
            "particularly in the domains of alignment and inter-\n",
            "pretability. GPT4All model weights and data are\n",
            "intended and licensed only for research purposes\n",
            "and any commercial use is prohibited. GPT4All\n",
            "is based on LLaMA, which has a non-commercial\n",
            "license. The assistant data is gathered from Ope-\n",
            "nAI’s GPT-3.5-Turbo, whose terms of use pro-\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"hibit developing models that compete commer-\n",
            "cially with OpenAI.\n",
            "References\n",
            "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\n",
            "Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\n",
            "Weizhu Chen. 2021. Lora: Low-rank adaptation of\n",
            "large language models.\n",
            "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\n",
            "Dubois, Xuechen Li, Carlos Guestrin, Percy\n",
            "Liang, and Tatsunori B. Hashimoto. 2023. Stan-\n",
            "ford alpaca: An instruction-following llama\n",
            "model. https://github.com/tatsu-lab/\n",
            "stanford_alpaca .\n",
            "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n",
            "Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\n",
            "Baptiste Rozi `ere, Naman Goyal, Eric Hambro,\n",
            "Faisal Azhar, Aurelien Rodriguez, Armand Joulin,\n",
            "Edouard Grave, and Guillaume Lample. 2023.\n",
            "Llama: Open and efficient foundation language\n",
            "models.\n",
            "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\n",
            "isa Liu, Noah A. Smith, Daniel Khashabi, and Han-\n",
            "naneh Hajishirzi. 2022. Self-instruct: Aligning lan-\n",
            "guage model with self generated instructions.\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"\n",
            "This report describes the development of GPT4All, a chatbot trained on a large corpus of assistant interactions including word problems, stories, multi-turn dialogue, and code. It outlines the data collection and curation process, training code, and final model weights, and also releases quantized 4-bit versions of the model for anyone to run on CPU.\n",
            "\n",
            " This project collected 806,199 prompt-response pairs using the GPT-3.5-Turbo OpenAI API between March 20th and 26th, 2023. Three public datasets were used to gather a diverse sample of questions/prompts. Data preparation and curation was based on the Stanford Alpaca project and Atlas was used to clean and remove malformed output.\n",
            "\n",
            "\n",
            "This paper discusses the process of cleaning a dataset of prompt-generation pairs for training a GPT-3.5-Turbo model. After prompting and producing malformed output, the total number of examples was reduced to 806,199. The Bigscience/P3 subset was removed due to its very low output diversity, leaving 437,605 prompt-generation pairs for the final training dataset. The dataset was then used to train several models finetuned from an instance of LLaMA 7B.\n",
            "\n",
            " This article describes the training of several models using LoRA (Hu et al., 2021) on 437,605 post-processed examples for four epochs. Detailed model hyper-parameters and training code can be found in the associated repository and model training log.\n",
            "\n",
            " This paper presents the development of a model, gpt4all-lora, which produces diverse and personalized generations related to personal health and wellness. The data, training code, and model weights have been released for the community to build upon. The model was developed with four days of work, $800 in GPU costs, and $500 in OpenAI API spend. A preliminary evaluation of the model was conducted using human evaluation data.\n",
            "\n",
            " This evaluation tested a model against the best openly available alpaca-lora model, provided by user chainyo on huggingface. Results showed that models finetuned on the collected dataset had much lower perplexity in the Self-Instruct evaluation compared to Alpaca. Further evaluation is needed, and the model can be tested locally on CPU.\n",
            "\n",
            " The authors release data and training details to accelerate open LLM research, particularly in the domains of alignment and interpretability. The GPT4All model weights and data are intended and licensed only for research purposes and any commercial use is prohibited. The assistant data is gathered from OpenAI's GPT-3.5-Turbo, whose terms of use prohibit commercial use.\n",
            "\n",
            " This article discusses the development of models that compete commercially with OpenAI. It references three models, LORA, Stanford Alpaca, and LLAMA, that are all open and efficient in their respective fields. Finally, it references Self-Instruct, which aligns language models with self-generated instructions.\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' This paper presents the development of GPT4All, a chatbot trained on a large corpus of assistant interactions, including word problems, stories, multi-turn dialogue, and code. The data, training code, and model weights have been released for the community to build upon. The models were trained using LoRA, Stanford Alpaca, and LLAMA, and evaluated using human evaluation data and Self-Instruct. The data and training details have been released for research purposes, and commercial use is prohibited.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_pdf(pdf_file_path):\n",
        "  loader=PyPDFLoader(pdf_file_path)\n",
        "  docs=loader.load()\n",
        "  text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "  texts=text_splitter.split_documents(docs)\n",
        "  chain=load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "  summary=chain.run(texts)\n",
        "  return summary"
      ],
      "metadata": {
        "id": "WnEGKOJiTMoz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_pdf_path=gr.components.Textbox(label=\"Provide the PDF File Path \")\n",
        "output_summary=gr.components.Textbox(label=\"Summary\")"
      ],
      "metadata": {
        "id": "OHMvdO0tTSS1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface=gr.Interface(\n",
        "    fn=summarize_pdf,\n",
        "    inputs=input_pdf_path,\n",
        "    outputs=output_summary,\n",
        "    title=\"PDF Summarizer\",\n",
        "    description=\"Provide PDF File Path to get the Summary\"\n",
        ").launch(share=True,debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "ay7yPhCnTbXM",
        "outputId": "50a81c88-727d-4d53-a9b9-28c3c094d3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://ae0af83a13b0d8a3a3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ae0af83a13b0d8a3a3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4u3_HxSTcw8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}